{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"ai_integration/backend/","title":"Backend Integration","text":"Stateless &amp; Serverless <ul> <li> <p>Stateless: Each request is independent and doesn\u2019t rely on any previous request's data stored on the server.</p> <ul> <li>Each AI request is independent \u2014 the model doesn\u2019t remember past requests unless you include them again in the new request (you must send the full context every time.).</li> <li>Chat-like memory is typically emulated by appending previous messages to the prompt (called prompt chaining or context windowing).</li> </ul> </li> <li> <p>Serverless: Doesn't mean \"no servers\". It means you don\u2019t manage the servers yourself. The cloud provider (like AWS, Google Cloud, or Vercel) handles infrastructure management for you.</p> <ul> <li>Serverless AI means you use AI services without running or managing your own model servers. The model is hosted by a provider (e.g., Together.ai), and you interact with it through APIs. You don\u2019t care where or how the model is running. It's abstracted away.</li> </ul> </li> </ul>"},{"location":"ai_integration/overview/","title":"\ud83e\udde0 AI Integration","text":"Workflow <pre><code>Frontend (HTML/JS)\n    |\n    |---&gt; API Request (user input/prompt)\n            |\n        Backend &amp; API Interaction (Python/Django)\n            |\n            LLM (local or platform)\n            |\n        AI Response\n            |\n    &lt;--- Response sent to frontend\n</code></pre> <p>We have integrated the Together API into our projects, enabling us to interact with over 200 different large language models (LLMs) for various purposes. Some of these models are available for free, while others are offered under paid plans.</p> <p>In this write-up, we'll walk you through the different components we implemented to create an AI chat system designed to combat imposter syndrome\u2014step by step.</p> <p>Before we begin, it\u2019s worth noting that Together.ai is just one of several powerful platforms available for LLM access. Other notable providers include:</p> <ul> <li>Hugging Face Inference API</li> <li>Replicate</li> <li>Groq</li> <li>Fireworks.ai (some offer free-tier usage)</li> <li>OpenAI (especially if you're considering a non-open-source route)</li> </ul>"},{"location":"ai_integration/overview/#integration","title":"\ud83d\udd27 Integration","text":"<p>Our AI chat system integration involves the following steps:</p> <ol> <li>Model Selection \u2013 Choosing appropriate LLMs based on response quality, latency, and licensing.</li> <li>Authentication &amp; API Setup \u2013 Registering API keys and securely managing them within the project.</li> <li>Backend Integration \u2013 Writing async functions to handle user input, call the API, and return results.</li> <li>Frontend Chat UI \u2013 Creating a user-friendly interface for smooth interaction with the AI.</li> <li>Streaming Responses \u2013 Implementing partial rendering as the AI generates responses (for real-time feedback).</li> <li>Fallback \u2013 Setting up fallback models</li> <li>Logging \u2013  Logging for analytics and debugging</li> </ol>"},{"location":"ai_integration/rag/","title":"RAG","text":""},{"location":"ai_integration/rag/#we-already-have","title":"\u2705 We Already Have:","text":"Component Purpose Backend Central logic, routes, and API communication layer Backend Integration Ties your app logic with AI providers (e.g., Together API, OpenAI) Context Handling Maintains chat/thread context across stateless API calls Caching Stores frequent results (e.g., for repeated prompts or static outputs) Rate Limit Prevents abuse and stays within API quota or billing plans Fallback Redirects requests to alternate models/providers if one fails Logging Captures API requests, errors, latency, etc., for debugging &amp; analytics Fine-Tuning Customizes model behavior for your domain or app via training"},{"location":"ai_integration/rag/#highly-recommended-additions","title":"\ud83e\udde9 Highly Recommended Additions:","text":""},{"location":"ai_integration/rag/#1-authentication-authorization","title":"1. Authentication &amp; Authorization","text":"<p>Protect your backend and APIs from unauthorized usage.</p> <ul> <li>Use OAuth2, JWT, or API keys.</li> <li>Role-based access for different types of users/admins.</li> </ul>"},{"location":"ai_integration/rag/#2-prompt-engineering-layer","title":"2. Prompt Engineering Layer","text":"<p>Centralize prompt templates and styles.</p> <ul> <li>Supports dynamic variables</li> <li>Helps unify prompt logic for different use cases</li> <li>Enables A/B testing of prompt strategies</li> </ul>"},{"location":"ai_integration/rag/#3-token-usage-tracking","title":"3. Token Usage Tracking","text":"<p>Essential for cost control and debugging.</p> <ul> <li>Monitor per-user, per-session, and per-model token usage</li> <li>Log tokens used per prompt/response</li> </ul>"},{"location":"ai_integration/rag/#4-error-retry-handling","title":"4. Error &amp; Retry Handling","text":"<p>Catch API timeouts, bad responses, or quota failures.</p> <ul> <li>Exponential backoff strategy for retries</li> <li>Graceful user-facing error messages</li> </ul>"},{"location":"ai_integration/rag/#5-model-abstraction-layer","title":"5. Model Abstraction Layer","text":"<p>Separate your app from any one provider.</p> <ul> <li>Create an interface that supports OpenAI, Together, Hugging Face, etc.</li> <li>Makes switching easier if a service goes down or pricing changes</li> </ul>"},{"location":"ai_integration/rag/#6-analytics-metrics","title":"6. Analytics &amp; Metrics","text":"<p>Capture usage patterns, latency, errors, etc.</p> <ul> <li>Helps you refine UX and backend performance</li> <li>Useful for growth metrics and user behavior</li> </ul>"},{"location":"ai_integration/rag/#7-session-management","title":"7. Session Management","text":"<p>Track multi-turn conversations or threaded interactions.</p> <ul> <li>Can be done via DB (Mongo/Postgres/Redis)</li> <li>Combine with <code>Context Handling</code></li> </ul>"},{"location":"ai_integration/rag/#optionaladvanced-ideas","title":"\ud83d\udca1 Optional/Advanced Ideas:","text":"Feature Purpose Streaming Responses Deliver partial responses in real time (e.g., like ChatGPT's typing) Multimodal Support Handle images/audio/video if models support them Model Selection UI Let users choose between models or temperature for experimentation RAG (Retrieval-Augmented Generation) Fetch relevant data from a knowledge base or DB to enhance response context Admin Dashboard Monitor requests, view logs, trigger fine-tunes or hard resets User Feedback Loop Collect and store \"thumbs up/down\" or edits for future model improvement"},{"location":"ai_integration/temperature/","title":"Temperature","text":""},{"location":"ai_integration/temperature/#what-is-temperature","title":"\ud83d\udd25 What is Temperature?","text":"<p>In large language models like those offered by Together.ai, OpenAI (GPT), or others, the temperature setting controls how random or deterministic the model\u2019s output will be.</p> <ul> <li>Low temperature (0.1 - 0.3) \u2192 More focused and predictable responses  </li> <li>High temperature (0.7 - 1.0) \u2192 More creative and diverse responses</li> </ul>"},{"location":"ai_integration/temperature/#effect-of-temperature","title":"\ud83c\udfa8 Effect of Temperature","text":""},{"location":"ai_integration/temperature/#low-temperature-01-03","title":"\ud83d\udd39 Low Temperature (0.1 - 0.3)","text":"<ul> <li>Output is consistent, logical, and factual</li> <li>Ideal for definitions, code generation, or concise factual answers</li> <li>Less variety \u2013 the model usually picks the most likely next word</li> </ul> <p>Example Prompt: \u201cWhat is the capital of France?\u201d Response: \u201cThe capital of France is Paris.\u201d</p>"},{"location":"ai_integration/temperature/#medium-temperature-04-07","title":"\ud83d\udd38 Medium Temperature (0.4 - 0.7)","text":"<ul> <li>Balances coherence with creativity</li> <li>Adds a bit of personality or flair to responses</li> <li>Good for general-purpose outputs with slight variation</li> </ul> <p>Example Prompt: \u201cGive me a motivational quote.\u201d Response: \u201cThe only limit to our realization of tomorrow is our doubts of today.\u201d</p>"},{"location":"ai_integration/temperature/#high-temperature-08-10","title":"\ud83d\udd3a High Temperature (0.8 - 1.0)","text":"<ul> <li>Output becomes more imaginative, varied, and sometimes unpredictable</li> <li>Excellent for storytelling, brainstorming, or creative writing</li> <li>May produce less accurate but more novel ideas</li> </ul> <p>Example Prompt: \u201cGive me a motivational quote.\u201d Response: \u201cDon\u2019t be afraid to fail, be afraid to never try. Life is a journey where courage is the compass.\u201d</p>"},{"location":"ai_integration/temperature/#when-to-use-different-temperatures","title":"\ud83c\udfaf When to Use Different Temperatures","text":"Temperature Use Case Examples Behavior Type 0.1 \u2013 0.3 Definitions, math, programming, FAQs Factual, repetitive, focused 0.4 \u2013 0.7 Educational content, summaries, advice Balanced, expressive, controlled 0.8 \u2013 1.0 Stories, poetry, ideas, jokes, creative prompts Free-form, experimental, novel"},{"location":"ai_integration/temperature/#how-temperature-affects-token-generation","title":"\ud83d\udcc8 How Temperature Affects Token Generation","text":"<ul> <li>Low Temp \u2192 The model chooses the highest probability tokens, creating stable, often repetitive text.</li> <li>High Temp \u2192 The model allows for lower probability tokens, resulting in more spontaneous and varied output.</li> </ul> <p>Temperature controls randomness and creativity.</p> <ul> <li> <p>Low \u2192 predictable, factual, repetitive</p> </li> <li> <p>Medium \u2192 balanced, coherent, but slightly creative</p> </li> <li> <p>High \u2192 creative, varied, and unpredictable</p> </li> </ul> <p>Use the temperature parameter based on the type of response you're seeking:</p> <ul> <li> <p>For consistent and focused answers, go for low temperature.</p> </li> <li> <p>For creative or diverse responses, go for higher temperature.</p> </li> </ul>"},{"location":"ai_integration/temperature/#try-it-yourself","title":"\ud83e\uddea Try It Yourself!","text":"<p>You can experiment with temperature settings via:</p> <ul> <li>Together.ai Playground </li> <li>OpenAI Playground </li> <li>API calls with a <code>temperature</code> parameter (e.g., in Python: <code>temperature=0.7</code>)</li> </ul>"},{"location":"ai_integration/tokens/","title":"Tokens","text":""},{"location":"ai_integration/tokens/#what-are-tokens","title":"\ud83d\udd0d What Are Tokens?","text":"Note <p>Tokenization can vary between models. For instance, models like LLaMA or Mistral may use different tokenization schemes compared to OpenAI's GPT models. Therefore, while these tools can provide estimates, they may not always reflect the exact token count used by Together.ai's models.\u200b</p> <p>A token can be a word, part of a word, punctuation mark, or even a space\u2014depending on the model's tokenization system. For example:</p> <ul> <li><code>\"Hello\"</code> is one token.</li> <li><code>\"ChatGPT is great!\"</code> might be split into: <code>[\"Chat\", \"GPT\", \"is\", \"great\", \"!\"]</code>.</li> </ul> <p>Even whitespace and punctuation count as tokens.</p> <p>Most modern LLMs, like those from OpenAI, use subword tokenization (often based on Byte Pair Encoding or BPE). This means a single word can be broken into smaller token parts. For example:</p> <ul> <li><code>\"unhappiness\"</code> might become: <code>\"un\"</code>, <code>\"happi\"</code>, <code>\"ness\"</code>.</li> </ul>"},{"location":"ai_integration/tokens/#why-do-tokens-matter","title":"\ud83e\udde0 Why Do Tokens Matter?","text":"<p>Tokens determine how much text you can send to or receive from an AI model, and how much it costs you.</p> <ul> <li>Input tokens (request): These are the tokens in your prompt (e.g., <code>\"Give me a motivational quote\"</code>).</li> <li>Output tokens (response): These are the tokens in the model\u2019s reply.</li> </ul> <p>So every message sent and received consumes tokens.</p>"},{"location":"ai_integration/tokens/#token-limits","title":"\ud83e\uddd1\u200d\ud83d\udcbb Token Limits","text":"<p>Each LLM has a maximum token limit per request. This includes both the input (your prompt) and the output (the model's reply).</p> <p>For example:</p> <p>\ud83e\udde0 <code>Llama-4-Maverick-17B-128E-Instruct-FP8</code> might have a token limit of 8,000 tokens (check your model's documentation to confirm).</p> <p>If your input is 200 tokens, you can receive up to 7,800 tokens in response.</p>"},{"location":"ai_integration/tokens/#token-pricing-quotas","title":"\u2696\ufe0f Token Pricing &amp; Quotas","text":"<p>For API-based services like:</p> <ul> <li>Together.ai</li> <li>OpenAI</li> <li>Hugging Face</li> </ul> <p>...you're often billed per token.</p>"},{"location":"ai_integration/tokens/#example","title":"Example:","text":"<p>If you send:</p> <ul> <li>50 tokens (prompt)</li> </ul> <p>And the model replies with:</p> <ul> <li>150 tokens</li> </ul> <p>Then you\u2019ve used 200 tokens in total for that API call.</p>"},{"location":"ai_integration/tokens/#how-does-this-affect-your-app","title":"\ud83d\udccf How Does This Affect Your App?","text":"<ul> <li>Short prompts (like a motivational quote) \u2192 minimal token usage.</li> <li>Long prompts or story generation \u2192 more tokens, higher costs, or risk of hitting the limit.</li> </ul>"},{"location":"ai_integration/tokens/#how-to-handle-tokens-in-your-app","title":"\ud83c\udff7\ufe0f How to Handle Tokens in Your App","text":"<p>Here are some tips to manage token usage efficiently:</p> <ul> <li>Track tokens: Use tools or SDK features to count tokens per request.</li> <li>Set limits: Use the <code>max_tokens</code> parameter to control how long responses can be.</li> <li>Restrict input: For example, cap user input to 150 tokens to keep usage within budget.</li> </ul> <p>You can use OpenAI\u2019s tokenizer tool to experiment and see how text is split into tokens.</p>"},{"location":"blog/","title":"Blog","text":""}]}